{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "z6wcfl5bnjflhfvqbnmzn"
   },
   "outputs": [],
   "source": [
    "%pip install pymorphy2\n",
    "%pip install examples/datasphere/stt_metrics-0.12-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "pdi41sd9ywggooxhxy7wlp"
   },
   "source": [
    "## Оценка качества STT моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7k58oggvwatlffly4zs7ft"
   },
   "source": [
    "Качество распознавания речи на имеющихся данных сильно зависит от выбора конкретной модели. Но чтобы можно было определить, какая из моделей лучше справляется с распознаванием под конкретный бизнес-кейс, требуется правильно построить систему оценки качества распознавания и определить соответствующие метрики.\n",
    "\n",
    "Наиболее популярной метрикой при оценке качества распознавания является метрика WER (Word Error Rate). Эта метрика оценивает похожесть полученного распознавания на некоторый \"эталонный\" пример, как правило получаемый с помощью разметки аудиозаписей с помощью асессоров. Проблема заключается в том, что значение этой метрики может очень сильно варьироваться не только от результатов распознавания, но также и от качества разметки аудиозаписей и самой методологии оценки.\n",
    "\n",
    "В качестве примера можно привести популярную фразу \"алло\", которая может быть размечена как минимум четырьмя различными способами: \"алло\", \"алле\", \"ало\", \"але\". Также качество может падать, если пытаться различать буквы \"ё\" и \"е\", которые эквивалентны с точки зрения большинства моделей распознавания. Наконец, качество может очень сильно зависеть от того, требуется ли нам различать различные формы одних и тех же слов (к примеру, род, падежи существительных, времена глаголов и т.п.).\n",
    "\n",
    "По этой причине мы решили предоставить пользователям свою небольшую библиотеку, которая позволит использовать метрики для оценки качества распознавания с учётом описанных особенностей. На текущий момент эта библиотека поддерживает вычисление метрики WER, однако в дальнейшем библиотеку планируется расширить и другими метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "wu2sr42o5dut1a288k6yo"
   },
   "outputs": [],
   "source": [
    "from stt_metrics import WER, ClusterReferences\n",
    "from stt_metrics.text_transform import Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "yc0598s63rmasktbb44rah"
   },
   "source": [
    "### Пример использования метрики WER\n",
    "\n",
    "Рассмотрим самый простой вариант использования метрики WER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "4y99e6d3ukju9e49pkxhsr"
   },
   "outputs": [],
   "source": [
    "reference = 'алло добрый день с моей карты только что списали крупную сумму денег хочу заблокировать её'\n",
    "hypothesis = 'але добрый день моей карты только что списал крупную суму денег хочу заблокировать ее'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "lvue2l49urq33ot90clyki"
   },
   "outputs": [],
   "source": [
    "wer = WER()\n",
    "wer_data = wer.get_metric_data(hyp=hypothesis, ref=reference)\n",
    "wer_value = wer.calculate_metric(wer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4m2uaxa0p3ubyimm0mwdie"
   },
   "source": [
    "`wer_data` &mdash; специальный объект, который хранит необходимую для вычисления WER информацию, а также предоставляет нам выравнивание двух текстов с указанием отличий. Последняя особенность может быть очень полезна при дальнейшем анализе отличий в распознавании и разметке.\n",
    "\n",
    "Так, на примере ниже мы видим, что значение метрики WER оказывается достаточно высоким. Однако многие ошибки для данного кейса оказываются довольно незначительными. Так, одна ошибка возникает из-за появившейся в разметке буквы \"ё\", ещё одна из ошибок связана с отличием в словах \"але\" и \"алло\", и ещё одна ошибка произошла из-за нераспознанного множественного числа глагола \"списал\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "02las68bbbxjbfcg023x5w"
   },
   "outputs": [],
   "source": [
    "print(wer_data)\n",
    "print(f'WER: {wer_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "f3bp6agkr19g0p60fawjh"
   },
   "source": [
    "### Избавление от ошибок\n",
    "\n",
    "#### Удаление артефактов\n",
    "\n",
    "От возможных ошибок первого типа довольно просто избавиться, если заранее провалидировать имеющуюся разметку аудио и избавиться от возможных артефактов. В данном случае такой препроцессинг оказывается довольно простым, хотя в общем случае артефакты могут быть и куда менее очевидными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "sifrl68bdficokkgpze9co"
   },
   "outputs": [],
   "source": [
    "def remove_artifacts(text):\n",
    "    return text.replace('ё', 'е')\n",
    "\n",
    "\n",
    "hypothesis, reference = remove_artifacts(hypothesis), remove_artifacts(reference)\n",
    "\n",
    "wer = WER()\n",
    "wer_data = wer.get_metric_data(hyp=hypothesis, ref=reference)\n",
    "wer_value = wer.calculate_metric(wer_data)\n",
    "\n",
    "print(wer_data)\n",
    "print(f'WER: {wer_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "c7bugxl9ftudswqm68cm6p"
   },
   "source": [
    "#### ClusterReferences\n",
    "\n",
    "Ошибки второго типа также могут быть довольно просто исключены, если явно указать наборы синонимичных фраз (ClusterReferences), которые следует воспринимать одинаково:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qs4zr12523lccn6a5gsy"
   },
   "outputs": [],
   "source": [
    "cr = ClusterReferences()\n",
    "cr.add_cluster(center='алло', aliases=['ало', 'але', 'алле'])\n",
    "\n",
    "wer = WER(cr=cr)\n",
    "wer_data = wer.get_metric_data(hyp=hypothesis, ref=reference)\n",
    "wer_value = wer.calculate_metric(wer_data)\n",
    "\n",
    "print(wer_data)\n",
    "print(f'WER: {wer_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "a23rpn5h2cn7ndxwj68c"
   },
   "source": [
    "#### Lemmatizer\n",
    "\n",
    "Наконец, можно также постараться избавиться и от ошибок третьего типа, если привести все имеющиеся слова к их леммам. Тем не менее, делать это также следует аккуратно, потому что в лемматизации также могут нуждаться и фразы из ClusterReferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "8hhjier6mvskvi6ndy0j9s"
   },
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "lemm_hypothesis, lemm_reference = lemmatizer.transform(hypothesis), lemmatizer.transform(reference)\n",
    "\n",
    "wer = WER(cr=cr)\n",
    "wer_data = wer.get_metric_data(hyp=lemm_hypothesis, ref=lemm_reference)\n",
    "wer_value = wer.calculate_metric(wer_data)\n",
    "\n",
    "print(wer_data)\n",
    "print(f'WER: {wer_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "virjbqdb3crjuxif3vjlh"
   },
   "source": [
    "### Выводы\n",
    "\n",
    "Если вам просто требуется проверить, что конкретная модель распознавания в среднем делает достаточно мало ошибок в словах, то простая версия метрики WER вполне может подойти вам.\n",
    "\n",
    "Если же вы используете эту метрику для сравнения поведения различных моделей на вашем бизнес-кейсе, то вам может также помочь:\n",
    "\n",
    "* Удаление из разметки и набор распознаваний явных артефактов, которые могут ухудшать значения используемых метрик, не влияя при этом на качество решения конкретной бизнес-задачи\n",
    "\n",
    "* Использование ClusterReference'ов для \"склейки\" одинаковых по сути распознаваний. Так можно склеить различные варианты распознавания фраз типа \"алло\", а также распознавания фраз с пробелами (наподобие \"контр страйк\" и \"контрстрайк\")\n",
    "\n",
    "* Лемматизация слов, если для нас не важны их окончания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "89d2tvzxin4f1g7k4vbv5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "7c29683c-86a5-421a-8a0c-90189ba53ee8"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
